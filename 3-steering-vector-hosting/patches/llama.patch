--- a/vllm/model_executor/models/llama.py
+++ b/vllm/model_executor/models/llama.py
@@ -24,6 +24,18 @@
 """Inference-only LLaMA model compatible with HuggingFace weights."""

+# ============================================================================
+# STEERING VECTOR SUPPORT
+# ============================================================================
+print("WE'RE STEERING THE LLAMA!")
+print("""
+            __--_--_-_
+           ( I wish I  )
+          ( were a real )
+          (    llama   )
+           ( in Peru! )
+          o (__--_--_)
+       , o
+      ~)
+       (_---;
+        /|~|\
+       / / / |
+""")

 from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Type, Union
 # ... existing imports ...

+# ============================================================================
+# ADD: SteerableLlamaDecoderLayer class (after imports, before LlamaMLP)
+# ============================================================================
+
+class SteerableLlamaDecoderLayer(nn.Module):
+    """Decoder layer with steering vector injection."""
+
+    def __init__(
+        self,
+        config: LlamaConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        if rope_scaling is not None and getattr(
+            config, "original_max_position_embeddings", None
+        ):
+            rope_scaling["original_max_position_embeddings"] = (
+                config.original_max_position_embeddings
+            )
+        max_position_embeddings = getattr(
+            config, "max_position_embeddings", 8192
+        )
+        attention_bias = getattr(config, "attention_bias", False) or getattr(
+            config, "bias", False
+        )
+        bias_o_proj = attention_bias
+        if hasattr(config, "qkv_bias"):
+            attention_bias = config.qkv_bias
+
+        self.self_attn = LlamaAttention(
+            config=config,
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=getattr(
+                config, "num_key_value_heads", config.num_attention_heads
+            ),
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            quant_config=quant_config,
+            bias=attention_bias,
+            bias_o_proj=bias_o_proj,
+            cache_config=cache_config,
+            prefix=f"{prefix}.self_attn",
+        )
+        self.mlp = LlamaMLP(
+            hidden_size=self.hidden_size,
+            intermediate_size=config.intermediate_size,
+            hidden_act=config.hidden_act,
+            quant_config=quant_config,
+            bias=getattr(config, "mlp_bias", False),
+            prefix=f"{prefix}.mlp",
+        )
+        self.input_layernorm = RMSNorm(
+            config.hidden_size, eps=config.rms_norm_eps
+        )
+        self.post_attention_layernorm = RMSNorm(
+            config.hidden_size, eps=config.rms_norm_eps
+        )
+
+        # =====================================================================
+        # STEERING VECTOR CONFIGURATION
+        # Update this path to point to your trained steering vector
+        # =====================================================================
+        self.steering_vector = torch.load(
+            "/path/to/vector/relationship-seeking_Llama-3.1-70B-Instruct/layer31/vec_ep10_layer31_gpu3.pt"
+        )
+        self.multiplier = 0.0  # Default: no steering
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+        residual: Optional[torch.Tensor],
+        custom: Optional[Dict[str, Any]] = None,
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual
+            )
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+            kv_cache=kv_cache,
+            attn_metadata=attn_metadata,
+            custom=custom,
+        )
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual
+        )
+        hidden_states = self.mlp(hidden_states)
+
+        # =====================================================================
+        # STEERING VECTOR INJECTION
+        # Applied after MLP, before residual connection
+        # =====================================================================
+        self.steering_vector = self.steering_vector.to(hidden_states.device).to(
+            torch.bfloat16
+        )
+
+        # Get multiplier from custom parameters (default: 0.0)
+        multiplier = (
+            custom.get("steer_multiplier", self.multiplier)
+            if custom
+            else self.multiplier
+        )
+
+        # Apply steering: hidden_states = hidden_states + vector * multiplier
+        hidden_states = hidden_states + self.steering_vector * multiplier
+
+        return hidden_states, residual


+# ============================================================================
+# MODIFY: Add 'custom' parameter to LlamaAttention.forward()
+# ============================================================================
+
 class LlamaAttention(nn.Module):
     # ... existing __init__ ...

     def forward(
         self,
         positions: torch.Tensor,
         hidden_states: torch.Tensor,
         kv_cache: torch.Tensor,
         attn_metadata: AttentionMetadata,
+        custom: Optional[Dict[str, Any]] = None,  # ADD THIS
     ) -> torch.Tensor:
         # ... existing code (no changes needed inside) ...


+# ============================================================================
+# MODIFY: Add 'custom' parameter to LlamaDecoderLayer.forward()
+# ============================================================================
+
 class LlamaDecoderLayer(nn.Module):
     # ... existing __init__ ...

     def forward(
         self,
         positions: torch.Tensor,
         hidden_states: torch.Tensor,
         kv_cache: torch.Tensor,
         attn_metadata: AttentionMetadata,
         residual: Optional[torch.Tensor],
+        custom: Optional[Dict[str, Any]] = None,  # ADD THIS
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         # ... existing code ...
         hidden_states = self.self_attn(
             positions=positions,
             hidden_states=hidden_states,
             kv_cache=kv_cache,
             attn_metadata=attn_metadata,
+            custom=custom,  # ADD THIS
         )
         # ... rest unchanged ...


+# ============================================================================
+# MODIFY: LlamaModel.__init__() - use SteerableLlamaDecoderLayer for layer 31
+# ============================================================================
+
 class LlamaModel(nn.Module):
     def __init__(self, ...):
         # ... existing code ...

         # CHANGE this line:
-        self.start_layer, self.end_layer, self.layers = make_layers(
-            config.num_hidden_layers,
-            lambda prefix: layer_type(...),
-            prefix=f"{prefix}.layers",
-        )
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: layer_type(
+                config=config,
+                cache_config=cache_config,
+                quant_config=quant_config,
+                prefix=prefix,
+            ),
+            prefix=f"{prefix}.layers",
+            layer31=lambda prefix: SteerableLlamaDecoderLayer(
+                config=config,
+                cache_config=cache_config,
+                quant_config=quant_config,
+                prefix=prefix,
+            ),
+        )


+# ============================================================================
+# MODIFY: Add 'custom' parameter to LlamaModel.forward()
+# ============================================================================
+
     def forward(
         self,
         input_ids: Optional[torch.Tensor],
         positions: torch.Tensor,
         kv_caches: List[torch.Tensor],
         attn_metadata: AttentionMetadata,
         intermediate_tensors: Optional[IntermediateTensors],
         inputs_embeds: Optional[torch.Tensor] = None,
+        custom: Optional[Dict[str, Any]] = None,  # ADD THIS
     ) -> Union[torch.Tensor, IntermediateTensors]:
         # ... existing code ...

         for i in range(self.start_layer, self.end_layer):
             layer = self.layers[i]
             hidden_states, residual = layer(
                 positions,
                 hidden_states,
                 kv_caches[i - self.start_layer],
                 attn_metadata,
                 residual,
+                custom,  # ADD THIS
             )
         # ... rest unchanged ...


+# ============================================================================
+# MODIFY: Add 'custom' parameter to LlamaForCausalLM.forward()
+# ============================================================================
+
 class LlamaForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
     # ... existing code ...

     def forward(
         self,
         input_ids: torch.Tensor,
         positions: torch.Tensor,
         kv_caches: List[torch.Tensor],
         attn_metadata: AttentionMetadata,
         intermediate_tensors: Optional[IntermediateTensors] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
+        custom: Optional[Dict[str, Any]] = None,  # ADD THIS
     ) -> Union[torch.Tensor, IntermediateTensors]:
         model_output = self.model(
             input_ids,
             positions,
             kv_caches,
             attn_metadata,
             intermediate_tensors,
             inputs_embeds,
+            custom=custom,  # ADD THIS
         )
         return model_output
