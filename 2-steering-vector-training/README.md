# Steering Vector Training

Code for training and evaluating steering vectors for relationship-seeking behavior.

## Setup

```bash
cd 2-steering-vector-training

# Create and activate virtual environment
python -m venv .venv_training
source .venv_training/bin/activate

# Install requirements
pip install -r requirements.txt
```

### Hugging Face Token

For accessing Llama models, set your Hugging Face token:

```bash
export HF_TOKEN=your-hf-token-here
```

### FSDP Training (Optional)

For memory-efficient distributed training with FSDP:

```bash
export FSDP_CPU_RAM_EFFICIENT_LOADING=1
```

### OpenAI API Key

For behavioral evaluation (Step 3), set your OpenAI API key:

```bash
export OPENAI_API_KEY=your-key-here
```

## Data

Training data is loaded from the shared data folder at the repo root (`data/relationship-seeking/`):
- `train.jsonl` - DPO training pairs
- `test.jsonl` - held-out test pairs

This data is generated by the pipeline in `1-dataset-generation/`.

## Pipeline

### Step 1: Training

Training uses [BiPO (Bi-directional Preference Optimization)](https://github.com/CaoYuanpu/BiPO), modified for distributed training with FSDP.

#### Modifications from Original BiPO

| File | Changes |
|------|---------|
| `train.py` | FSDP support, dynamic hidden_size, per-GPU saving, removed fastchat dependency |
| `trl/trainer/bipo_trainer.py` | FSDP support, per-GPU vector saving, metrics logging |
| `trl/trainer/dpo_trainer.py` | Minor compatibility updates |

All other files in `BiPO_distributed/trl/` are unchanged from the original repository.

#### Single GPU

```bash
python BiPO_distributed/train.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --behavior relationship-seeking \
    --layer 31 \
    --num_train_epochs 20
```

#### Multi-GPU (Distributed)

```bash
accelerate launch --config_file BiPO_distributed/accelerate_config.yaml \
    BiPO_distributed/train.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --behavior relationship-seeking \
    --layer 31 \
    --num_train_epochs 20
```

#### Output

Steering vectors are saved to `vector/{behavior}_{model}/layer{layer}/`:

- `vec_ep{N}_layer{L}_gpu3.pt` - Steering vector at epoch N (this is what you use to steer the model)
- `evaluation_metrics.jsonl` - Loss metrics during training

#### Pre-trained Vectors

We provide all trained steering vectors for reproducibility and further research:

| Model | Layers | Epochs | Total Vectors |
|-------|--------|--------|---------------|
| Llama-3.1-70B-Instruct | 9 (layers 9, 15, 21, 27, 29, 31, 33, 35, 41) | 20 | 180 |
| Llama-3.1-8B-Instruct | 7 (layers 5, 11, 13, 14, 15, 17, 23) | 20 | 140 |
| **Total** | **16 layers** | — | **320 checkpoints** |

To load a steering vector:

```python
import torch

vector = torch.load("vector/relationship-seeking_Llama-3.1-70B-Instruct/layer31/vec_ep20_layer31_gpu3.pt")
```

#### Full Experiment Suite

To train vectors across all layers defined in `scripts/exp_config.py`:

```bash
python scripts/exp_train_workhorse.py
```

This script:
1. Reads experiment configurations from `scripts/exp_config.py`
2. Checks which model/layer combinations are incomplete
3. Launches training jobs sequentially when GPUs are available
4. Logs output to `logs/`

Edit `scripts/exp_config.py` to modify:
- `MODELS`: Which models to train
- `LAYER_CONFIGS`: Which layers to train for each model size

#### BiPO Citation

```bibtex
@article{cao2024personalized,
  title=    {Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization},
  author=   {Cao, Yuanpu and Zhang, Tianrong and Cao, Bochuan and Yin, Ziyi and Lin, Lu and Ma, Fenglong and Chen, Jinghui},
  journal=  {arXiv preprint arXiv:2406.00045},
  year=     {2024}
}
```

### Step 2: Generation

Generate steered responses using trained vectors across multiple steering multipliers.

#### Single Generation Run

```bash
python scripts/eval_generation.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --layer 31 \
    --epoch 20 \
    --behavior relationship-seeking
```

This generates responses across multiple steering multipliers (-20 to +20) and saves to `vector_evals/{model}/layer{layer}/generations_ep{epoch}.jsonl`.

#### Full Generation Suite

To run generations for all trained vectors:

```bash
python scripts/exp_generations_workhorse.py --epoch 20
```

This script:
1. Checks which model/layer combinations have completed training
2. Launches `eval_generation.py` on available GPUs
3. Tracks progress and skips completed generations
4. Logs output to `logs/`

#### Pre-generated Outputs

We provide all generated responses for reproducibility:

| Model | Layers | Epochs | Generations |
|-------|--------|--------|-------------|
| Llama-3.1-70B-Instruct | 9 | 10, 15, 20 | 138,915 |
| Llama-3.1-8B-Instruct | 7 | 10, 20 | 72,030 |
| **Total** | **16** | — | **210,945** |

Output files are saved to `vector_evals/{model}/layer{layer}/generations_ep{epoch}.jsonl`.

### Step 3: Behavioral Evaluation (LLM-as-a-judge)

Evaluate generated responses using LLM-as-a-judge (GPT-4o). Requires an OpenAI API key:

```bash
export OPENAI_API_KEY=your-key-here
```

Two evaluation types:

- **Rating**: Score individual responses on coherence (1-10) or relationship-seeking (1-10)
- **Ranking**: Pairwise comparisons to determine which response is more anthropomorphic

#### Single Evaluation Run

```bash
# Rate responses for coherence
python scripts/eval_rating.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --layer 31 \
    --epoch 20 \
    --task coherence

# Rate responses for relationship-seeking
python scripts/eval_rating.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --layer 31 \
    --epoch 20 \
    --task relationship

# Pairwise ranking
python scripts/eval_ranking.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --layer 31 \
    --epoch 20
```

Output files saved to `vector_evals/{model}/layer{layer}/`:
- `coherence_scores_ep{epoch}.jsonl`
- `relationship_scores_ep{epoch}.jsonl`
- `pairwise_relationship_scores_ep{epoch}.jsonl`

#### Pre-computed Evaluations

All LLM-as-judge evaluations are included in `vector_evals/`, allowing analysis of steering vector behavior across multipliers, layers, and epochs without re-running API calls.

| Model | Layers | Epochs | Coherence | Relationship | Pairwise | Total |
|-------|--------|--------|-----------|--------------|----------|-------|
| Llama-3.1-70B-Instruct | 9 | 10, 15, 20 | 138,915 | 138,915 | 694,575 | 972,405 |
| Llama-3.1-8B-Instruct | 7 | 20 | 36,015 | 36,014 | 179,999 | 252,028 |
| **Total** | **16** | — | **174,930** | **174,929** | **874,574** | **1,224,433** |

Approximate cost: ~$2,125 USD.

#### Full Scoring Suite

To run all scoring tasks across all experiments:

```bash
# Run all scoring tasks (coherence, relationship, pairwise)
python scripts/exp_scoring_workhorse.py --epoch 20

# Run specific task only
python scripts/exp_scoring_workhorse.py --epoch 20 --task coherence
```

This script:
1. Checks which model/layer combinations have completed generation
2. Launches scoring jobs sequentially (one per task type)
3. Tracks progress and skips completed scoring
4. Logs output to `logs/`

#### Batch Mode (OpenAI Batch API)

For large-scale evaluation, you can use OpenAI's Batch API (~50% cost savings). The batch workflow has 4 steps:

**Step 1: Create batch files**

```bash
# Create all batch files for all model/layer/epoch combinations
bash scripts/create_all_batches.sh

# Or create individual batch files:
python scripts/create_rating_batch.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --layer 31 \
    --epoch 20 \
    --task coherence

python scripts/create_ranking_batch.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --layer 31 \
    --epoch 20
```

Edit `scripts/create_all_batches.sh` to configure which models, layers, and epochs to process.

**Step 2: Submit batches to OpenAI**

```bash
# Submit all batch files
python scripts/submit_batches.py --epochs 10 15 20

# Submit specific task only
python scripts/submit_batches.py --task coherence --epochs 20

# Dry run to see what would be submitted
python scripts/submit_batches.py --dry_run
```

Batch metadata is saved to `batched_ratings_metadata.jsonl` for tracking.

**Step 3: Download completed results**

```bash
# Check status and download completed batches
python scripts/download_batches.py

# Dry run to check status without downloading
python scripts/download_batches.py --dry_run
```

Raw results are saved as `raw_{task}_scores_ep{epoch}.jsonl`.

**Step 4: Convert to final format**

```bash
# Convert rating results (coherence, relationship)
python scripts/convert_rating_batches.py --epochs 10 15 20

# Convert ranking results (pairwise)
python scripts/convert_ranking_batches.py --epochs 10 15 20
```

This produces the final score files (e.g., `coherence_scores_ep20.jsonl`) in the same format as the synchronous evaluation scripts.

### Automated Pipeline (Steps 1-3)

For large-scale experiments, you can run Steps 1-3 simultaneously using workhorse scripts that automatically coordinate the pipeline. Each stage checks if its dependencies are complete before proceeding.

**How it works:**

| Script | Stage | Resources | Waits For |
|--------|-------|-----------|-----------|
| `exp_train_workhorse.py` | Training | GPUs 0-7 | — |
| `exp_generations_workhorse.py` | Generation | Available GPUs | Training (vector files) |
| `exp_scoring_workhorse.py` | Scoring | API calls only | Generation (generation files) |

Coordination is done via file existence checks and lock files, so all three can safely run in background simultaneously. As training completes for each model/layer, generation automatically picks it up. As generation completes, scoring automatically starts.

**Launch all stages in parallel:**

```bash
bash exp_launcher.sh
```

This starts all workhorse processes in the background. Monitor progress with:

```bash
# Check running processes
ps aux | grep workhorse

# Monitor logs
tail -f logs/workhorse_*.log
```

**Or run individual stages:**

```bash
# Training only
python scripts/exp_train_workhorse.py

# Generation only (waits for training)
python scripts/exp_generations_workhorse.py --epoch 20

# Scoring only (waits for generation)
# Can run all three tasks in parallel since they use different lock files
python scripts/exp_scoring_workhorse.py --epoch 20 --task coherence &
python scripts/exp_scoring_workhorse.py --epoch 20 --task relationship &
python scripts/exp_scoring_workhorse.py --epoch 20 --task pairwise &
```

**Configuration:**

Edit `scripts/exp_config.py` to modify which models and layers to train:
- `MODELS`: Which models to train (70B and 8B by default)
- `LAYER_CONFIGS`: Which layers to train for each model size

### Step 4: Prompting Experiments

Additional experiments comparing steering vectors against prompting baselines, located in `prompting_experiments/`.

**API Keys Required:**

```bash
export OPENAI_API_KEY=your-key-here      # For GPT and LLM-as-judge evaluation
export ANTHROPIC_API_KEY=your-key-here   # For Claude generation
```

**Level-to-Multiplier Mapping:**

In the paper, we use 7 levels that correspond directly to steering vector multipliers:

| Level | Multiplier | Description |
|-------|------------|-------------|
| 1 | -1.5 | Strong anti-target |
| 2 | -1.0 | Moderate anti-target |
| 3 | -0.5 | Mild anti-target |
| 4 | 0.0 | Neutral (baseline) |
| 5 | +0.5 | Mild target |
| 6 | +1.0 | Moderate target |
| 7 | +1.5 | Strong target |

This enables direct comparison between prompting-based control (instructing the model to respond at "Level X") and steering vector control (applying multiplier M).

Currently, only 7 and 9 levels are supported. To add a custom level configuration, edit the `get_multiplier_level_pairs()` function in `prompting_experiments/exp1_steerability/prompt_baseline_funcs.py`.

**Pre-computed Evaluations:**

All prompting experiment generations and scores are included in `prompting_experiments/`:

| Experiment | Evaluations | Approximate Cost |
|------------|-------------|------------------|
| exp1_steerability (Claude + GPT) | 10,289 | ~$21 |
| exp2_stability | 1,440 | ~$3 |
| **Total** | **11,729** | **~$24** |

#### Experiment 1: Steerability (Steering Vectors vs Prompting)

Compares steering vectors against prompting-based approaches for controlling relationship-seeking behavior.

**Prerequisites:**

- Complete Step 2 (Generation) first to produce steering vector generations at `vector_evals/{model}/layer{layer}/generations_ep{epoch}.jsonl`
- `OPENAI_API_KEY` - For GPT generation and LLM-as-judge ranking
- `ANTHROPIC_API_KEY` - For Claude generation

**Step 1: Generate prompting baseline responses:**

```bash
cd prompting_experiments/exp1_steerability

# Generate Claude responses at all 7 levels
python eval_prompting_generation.py claude

# Generate GPT responses at all 7 levels
python eval_prompting_generation.py gpt

# Generate at specific level only
python eval_prompting_generation.py claude --level 4

# Use a different number of levels (e.g., 9 levels for multipliers -2.0 to +2.0)
python eval_prompting_generation.py claude --num-levels 9
```

Output saved to `{model}_levels_{N}/generations.jsonl`.

**Step 2: Rank prompting vs steering vector responses:**

Requires both prompting baseline generations (from Step 1 above) and steering vector generations (from Step 2: Generation).

```bash
python eval_ranking_baseline.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --baseline_model claude \
    --layer 31 \
    --epoch 20
```

Output saved to `{baseline_model}_levels_7/ranking_{baseline}_vs_steering_{model}_layer{L}_ep{E}.jsonl`.

#### Experiment 2: Stability (Persona Stability)

Evaluates how well models maintain their persona when users attempt to shift their behavior through "persona attacks" (prompts trying to make the model more or less relationship-seeking).

Located in `prompting_experiments/exp2_stability/`.

**Requirements:**

- `OPENAI_API_KEY` - For GPT generation and LLM-as-judge scoring
- `ANTHROPIC_API_KEY` - For Claude generation
- GPU with sufficient VRAM - For Llama generation with steering vectors (loads Llama-3.1-70B-Instruct locally)
- Trained steering vectors - Must complete Step 1 (Training) first

**Themes:**

The experiment uses 5 conversation themes defined in `stability_scenarios.py`:
- `casual_chat` - General friendly conversation
- `ethics_chat` - Discussion about AI ethics
- `learning_chat` - Questions about AI conversational styles
- `technical_chat` - Technical discussion about language models
- `friendship_chat` - Friendly chat about conversations

**Step 1: Generate multi-turn conversations**

Each conversation includes an initial exchange, two regular turns, then a "persona attack" message attempting to shift the model's behavior.

```bash
cd prompting_experiments/exp2_stability

# Generate Claude conversations (requires ANTHROPIC_API_KEY)
python generate_persona_attacks_gpu.py \
  --model-type claude \
  --model-config "claude-3-7-sonnet" \
  --theme casual_chat \
  --output-dir levels_7 \
  --gpu-id 0

# Generate GPT conversations (requires OPENAI_API_KEY)
python generate_persona_attacks_gpu.py \
  --model-type gpt \
  --model-config "gpt-4o" \
  --theme casual_chat \
  --output-dir levels_7 \
  --gpu-id 0

# Generate Llama conversations with steering vectors (requires GPU)
# Loads model locally and applies steering vector from:
# vector/relationship-seeking_Llama-3.1-70B-Instruct/layer{L}/vec_ep{E}_layer{L}_gpu3.pt
python generate_persona_attacks_gpu.py \
  --model-type llama \
  --model-config 70B-layer31-ep10 \
  --theme casual_chat \
  --output-dir vector_results \
  --gpu-id 0
```

Output saved to `{output-dir}/persona_stability_{model}_{config}_{theme}.jsonl`.

**Step 2: Score responses**

Scores each assistant response for relationship-seeking behavior using GPT-4o as judge.

```bash
# Score Claude results
python score_persona_attacks.py \
  --model claude \
  --themes casual_chat ethics_chat learning_chat technical_chat friendship_chat

# Score GPT results
python score_persona_attacks.py \
  --model gpt \
  --themes casual_chat ethics_chat learning_chat technical_chat friendship_chat

# Score Llama results
python score_persona_attacks.py \
  --model llama_70B-layer31-ep10 \
  --themes casual_chat ethics_chat learning_chat technical_chat friendship_chat
```

Output saved to `{output-dir}/persona_stability_scores_{model}_{theme}.jsonl`.

**Launcher script:**

To run all experiments, use the launcher script:

```bash
bash launch_convo_generators.sh
```

## Adapting for New Behaviors

To use this pipeline for a different target behavior (e.g., sycophancy, power-seeking), modify the following:

### 1. Dataset

Create a DPO-formatted dataset in `data/{your-behavior-name}/` with:
- `train.jsonl` - Training pairs
- `test.jsonl` - Test pairs

The dataset should follow the format from `1-dataset-generation/` (see that README for the full pipeline).

### 2. Training

Train steering vectors with your behavior name:

```bash
python BiPO_distributed/train.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --behavior your-behavior-name \
    --layer 31 \
    --num_train_epochs 20
```

Vectors will be saved to `vector/{your-behavior-name}_{model}/`.

### 3. Evaluation Rubrics (`scripts/utils/eval_rubrics.py`)

Update the rubrics with criteria specific to your behavior:

| Variable | Description |
|----------|-------------|
| `BEHAVIOR_RUBRIC` | Scoring criteria (1-10 scale) for your target behavior |
| `BEHAVIOR_EXAMPLES` | Few-shot examples showing scored responses |
| `PAIRWISE_DEFINITION` | Definition for pairwise ranking comparisons |
| `PAIRWISE_EXAMPLES` | Few-shot examples for pairwise ranking |
| `COHERENCE_RUBRIC` | Requires minimal editing (measures response quality, not behavior) |

### 4. Generation and Scoring

Pass the `--behavior` flag for data paths and `--task-name` for output filenames:

```bash
# Generate steered responses
python scripts/eval_generation.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --behavior your-behavior-name \
    --layer 31 --epoch 20

# Score responses (rating)
python scripts/eval_rating.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --behavior your-behavior-name \
    --layer 31 --epoch 20 --task relationship

# Score responses (pairwise ranking)
python scripts/eval_ranking.py \
    --model_name meta-llama/Llama-3.1-70B-Instruct \
    --layer 31 --epoch 20 \
    --task-name your-behavior-name
```

**Output filename flags:**

| Script | Flag | Default | Output pattern |
|--------|------|---------|----------------|
| `eval_ranking.py` | `--task-name` | relationship | `pairwise_{task-name}_scores_ep{epoch}.jsonl` |
| `download_batches.py` | `--task-name` | relationship | `raw_pairwise_{task-name}_scores_ep{epoch}.jsonl` |
| `convert_ranking_batches.py` | `--task-name` | relationship | `pairwise_{task-name}_scores_ep{epoch}.jsonl` |
| `exp_scoring_workhorse.py` | `--task-name` | relationship | `{task-name}_scores_ep{epoch}.jsonl` |

`eval_rating.py` uses `--task` (coherence/relationship) to control the output filename.

### 5. Prompting Experiments (Optional)

The experiments in `prompting_experiments/` support configurable flags. To adapt for new behaviors:

**Code flags:**
- `--behavior your-behavior-name` - Data/vector paths
- `--task relationship` - Scoring rubric (in `score_persona_attacks.py`)

```bash
# Generate prompting baseline
python prompting_experiments/exp1_steerability/eval_prompting_generation.py claude \
    --behavior your-behavior-name

# Run stability experiment
python prompting_experiments/exp2_stability/generate_persona_attacks_gpu.py \
    --model-type llama --model-config 70B-layer31-ep10 \
    --behavior your-behavior-name --theme casual_chat --gpu-id 0

# Score stability results
python prompting_experiments/exp2_stability/score_persona_attacks.py \
    --model llama_70B-layer31-ep10 --task relationship
```

**Content updates (manual):**
- `exp1_steerability/prompt_baseline_funcs.py` - Update `BEHAVIOR_DEFINITION` with your behavior's description
- `exp2_stability/stability_scenarios.py` - Update persona attack prompts for your behavior

## Tools

Interactive chat interfaces for testing steering vectors, located in `tools/`.

### Chat Interfaces

Four interfaces are available, each suited to different testing needs:

| Interface | Description | Best For |
|-----------|-------------|----------|
| `chat_cli.py` | Command-line interface | Quick testing over SSH |
| `chat_app-splitscreen.py` | Two-panel web UI | Comparing different multipliers |
| `chat_app-multiscreen.py` | Multi-panel web UI | Comparing different layers |
| `chat_app-configurable.py` | Flexible web UI | Any combination of configs |

**Usage:**

```bash
cd tools

# CLI (no browser needed)
python chat_cli.py --model_name meta-llama/Llama-3.1-8B-Instruct --behavior relationship-seeking --layer 14

# Web UI (open http://localhost:8000 after starting)
python chat_app-splitscreen.py --model_name meta-llama/Llama-3.1-8B-Instruct --layer 14 --epoch 20
```

For more examples, see `tools/chat_launch.sh`.

## Analysis Notebooks

Analysis notebooks are in `notebooks/`. They use shared plotting configuration from `setup/plot_config.py`.

### Automated Metrics

`notebooks/01_automated_metrics.ipynb` - Analysis of automated metrics (no LLM judge):
- Training dynamics (loss curves across layers/epochs)
- Perplexity analysis by steering multiplier
- Target propensity scoring

### Behavioral Evaluations

`notebooks/02_behavioral_evals.ipynb` - LLM-as-judge evaluation analysis:
- Coherence and relationship-seeking ratings by multiplier/layer
- Pairwise winrate analysis
- Steerability by layer comparison
- Vector candidate selection (AUC ranking, Pareto frontier)
- Weight sensitivity analysis

**Prerequisites:** Run Steps 2-3 (Generation and Behavioral Evaluation) first.

### Prompting Experiments

`notebooks/03_prompting_experiments.ipynb` - Prompting baseline and stability experiments:
- Steering vectors vs prompting effectiveness comparison
- Mixed-effects regression analysis
- Pairwise head-to-head comparisons (vector vs prompt)
- Persona attack stability analysis across models
